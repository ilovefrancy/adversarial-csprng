\relax 
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{kelsey1998cryptanalytic}
\citation{deng2017developments}
\citation{russel2009artificial}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\citation{wen2014exponential}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{2}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{3}}
\citation{barker2007recommendation}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{donald1998art}
\citation{katz2014introduction}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{google2018automl}
\citation{amazon2018aws}
\citation{microsoft2018azure}
\citation{forbes2016short}
\citation{abadi2016learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{goodfellow2014generative}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\citation{kelsey1998cryptanalytic}
\citation{goodfellow2014generative}
\citation{menezes1996handbook}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims and Motivation}{2}}
\newlabel{subsection:aims}{{1.1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Approach, Scope, and Assumptions}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Contributions and Findings}{3}}
\@writefile{tdo}{\contentsline {todo}{possibly remove because touches upon complicated concepts}{3}}
\pgfsyspdfmark {pgfid1}{4661699}{48303296}
\pgfsyspdfmark {pgfid4}{36577345}{48318041}
\pgfsyspdfmark {pgfid5}{38002753}{48049344}
\citation{klimov2002analysis}
\citation{donald1998art}
\citation{bennett2009randomness}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Numbers Sequences and Generators}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Random Bits, Random Numbers, and Entropy}{4}}
\citation{rukhin2001statistical}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{barker2007recommendation}
\citation{cover2012elements}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A discrete uniform probability distribution for an arbitrary experiment with 10 possible outcomes.}}{5}}
\newlabel{figure:uniform_distribution}{{2.1}{5}}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{bierhorst2018experimentally}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\newlabel{eq:entropy}{{2.1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random Number Generators}{6}}
\citation{ferguson2010cryptography}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{matsumoto1998mersenne}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{donald1998art}
\citation{ferguson2010cryptography}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A high-level view of the operation of a PRNG \cite  {kelsey1998cryptanalytic}.}}{7}}
\newlabel{figure:prng_high_level}{{2.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Pseudo-Random Number Generators}{7}}
\newlabel{subsection:prngs}{{2.1.3}{7}}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{anderson2010security}
\citation{anderson2010security}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Random Numbers and Cryptography}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Cryptographic Applications of Random Numbers}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Cryptographically Secure Pseudo-Random Number Generators}{8}}
\newlabel{subsection:crypto_requirements}{{2.2.2}{8}}
\citation{menezes1996handbook}
\citation{anderson2010security}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The probability distribution of a random variable representing the number of 1s in a truly random binary sequence of length 50. Values at the tails of the distribution, such as 2 and 48, could be chosen as critical values; farther results are so unlikely that, upon observing them, one could justifiably question whether the sequence really is random}}{9}}
\newlabel{figure:distribution}{{2.3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Testing Number Sequences for Randomness}{9}}
\newlabel{subsection:testing_prngs}{{2.2.3}{9}}
\citation{lavasani2009practical}
\citation{rukhin2001statistical}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{russel2009artificial}
\citation{russel2009artificial}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introduction to Neural Networks}{10}}
\newlabel{subsection:neural_intro}{{2.3.1}{10}}
\newlabel{eq:activation}{{2.2}{10}}
\newlabel{eq:activation_vector}{{2.3}{10}}
\newlabel{eq:neural_net_composition}{{2.4}{10}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Schematic of the computation performed by a unit, drawn in accordance to the definitions given by Russel and Norvig \cite  [p.728]{russel2009artificial}. A weighted sum $in_j$ of the unit's inputs is computed, and the resulting value becomes the argument of the activation function $g$. The activation's output is the node's output. This schematic corresponds to equation \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `eq:activation' on page 11 undefined}.}}{11}}
\newlabel{figure:neural_unit}{{2.4}{11}}
\@writefile{tdo}{\contentsline {todo}{explain hyperparameter optimization}{11}}
\pgfsyspdfmark {pgfid6}{33866153}{32681026}
\@writefile{tdo}{\contentsline {todo}{mention recurrent networks}{11}}
\pgfsyspdfmark {pgfid11}{33866153}{32681026}
\pgfsyspdfmark {pgfid9}{36577345}{32695771}
\pgfsyspdfmark {pgfid10}{38002753}{32427074}
\pgfsyspdfmark {pgfid14}{36577345}{24464449}
\pgfsyspdfmark {pgfid15}{38002753}{24195752}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Learning in Neural Networks}{11}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{tdo}{\contentsline {todo}{image of stable vs wiggly loss}{12}}
\pgfsyspdfmark {pgfid16}{27343388}{44467247}
\pgfsyspdfmark {pgfid19}{36577345}{44481992}
\pgfsyspdfmark {pgfid20}{38002753}{44213295}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Activation Functions}{12}}
\newlabel{eq:relu}{{2.5}{12}}
\newlabel{eq:leakyrelu}{{2.6}{12}}
\citation{sharma2017activation}
\citation{sharma2017activation}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Left: the ReLU function. Right: the LeakyReLU function. Note the zero gradient for negative inputs on the left, and how this problem is adressed on the right. Image courtesy of \cite  {sharma2017activation}.}}{13}}
\newlabel{figure:relu}{{2.5}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Fully Connected Feed-Forward Networks}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Convolutional Neural Networks}{13}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{albelwi2017framework}
\citation{albelwi2017framework}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Simple example of the structure of a feedforward network. The network has two hidden layers, for a total depth of 4. The network's width is 6, as given by the size of the hidden layers. Image drawn using draw.io \cite  {jgraph2018draw}}}{14}}
\newlabel{figure:feedforward}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Generative Adversarial Networks}{14}}
\newlabel{subsection:generativeadversarial}{{2.3.6}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A representation of the connectivity between the units of a 1-dimensional input layer (white) and the filters in a convolutional layer (yellow), with each convolutional unit's input weights shown in the squares. In both images, the kernel size is 3; on the left the stride of the layer is 1, while on the right it is 2. Furthermore, the convolutional layer on the left has two filters. Note how both filters are connected to the input in the same way, and how each filter has its own set of input weights. The image is based on the CS231n lecture notes by Andrej Karpathy \cite  [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers]{karpathy2017cs231n} and was drawn with draw.io \cite  {jgraph2018draw}.}}{15}}
\newlabel{figure:convolution1d}{{2.7}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A convolutional layer preserves the dimensions of its input matrix, but produces an output with a larger depth dimensions depending on the number of filters. This is followed by pooling layers, which down-sample the data in the width and height dimensions. Image courtesy of \cite  {albelwi2017framework}.}}{15}}
\newlabel{figure:conv_pooling}{{2.8}{15}}
\citation{goodfellow2016nips.}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\citation{wen2014exponential}
\newlabel{eq:gan_train}{{2.7}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Related Work}{16}}
\newlabel{section:related_work}{{2.4}{16}}
\citation{desai2012pseudo}
\citation{desai2011pseudo}
\citation{tirdad2010hopfield}
\citation{abadi2016learning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The conceptual structure of a generative adversarial network. The outputs of the generator are fed into the discriminator along with samples from the original dataset; the performance of the discriminator is factored into the loss function of the generator. Image drawn with draw.io \cite  {jgraph2018draw}.}}{17}}
\newlabel{figure:gan}{{2.9}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Papers on Using Neural Networks as Pseudo-Random Number Generators}{17}}
\@writefile{tdo}{\contentsline {todo}{more detail because this is interesting}{17}}
\pgfsyspdfmark {pgfid21}{27493385}{17852870}
\pgfsyspdfmark {pgfid24}{36577345}{17867615}
\pgfsyspdfmark {pgfid25}{38002753}{17598918}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Learning to Protect Communications with Adversarial Neural Cryptography}{17}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design and Implementation}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Conceptual Design}{19}}
\newlabel{section:conceptual_design}{{3.1}{19}}
\newlabel{eq:conceptual_prng}{{3.1}{19}}
\citation{russel2009artificial}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The conceptual difference in the common implementation of PRNGs (top) and the implementation using GANs in this work (bottom). Image produced using draw.io \cite  {jgraph2018draw}.}}{20}}
\newlabel{figure:conceptual_difference}{{3.1}{20}}
\citation{karpathy2017cs231n}
\citation{kingma2014adam}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The two approaches differ at the conceptual level in the the discriminative approach (left) requires a source of true randomness which it attempts to emulate, while the predictive approach (right) is an even purer ``game" between the two networks, with no side inputs. Image produced using draw.io \cite  {jgraph2018draw}.}}{21}}
\newlabel{figure:approach_comparison}{{3.2}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of the generator. Each fully connected feed-forward layer's activation function (blue) is shown; the output layer (red) differs from the other layers in that it does not use the leaky ReLU activation, but rather a modulus function. Image produced using draw.io \cite  {jgraph2018draw}.}}{21}}
\newlabel{figure:architecture_generator}{{3.3}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Generative Model}{21}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{numpy}
\citation{numpy}
\citation{tensorflow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Convolutional discriminator architecture. The output of the generator is convolved multiple times in order to extract higher-level features from the sequence; this is followed by pooling to reduce the output size, and fully connected feed-forward layers to produce the final classification output. Image produced using draw.io \cite  {jgraph2018draw}.}}{22}}
\newlabel{figure:architecture_conv}{{3.4}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Convolutional predictor architecture. The design is shared with the discriminator, but the size of the input sequence and the meaning of the output scalar are different. The predictor produces a guess for the last value in the generator's output based on the previous values. Image produced using draw.io \cite  {jgraph2018draw}.}}{22}}
\newlabel{figure:architecture_conv}{{3.5}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Discriminative Model}{22}}
\citation{bhatia2017why}
\citation{tensorflow2018intro}
\citation{tensorflow2018intro}
\citation{tensorflow2018graphs}
\citation{tensorflow2018intro}
\citation{git2018}
\citation{docker2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Predictive Model}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation Technologies}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to TensorFlow}{23}}
\citation{jetbrains2018pycharm}
\citation{tensorflow2018tutorials}
\citation{tensorflow2018tfgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Supporting Technologies}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Software Design}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}System Parameterization}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Generative Model}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Discriminative and Predictive Models}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Custom Tensor Operations}{27}}
\newlabel{subsection:custom_ops}{{3.3.4}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Obtaining Training Data}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Defining and Training the Discriminative GAN}{28}}
\newlabel{subsection:training_disc}{{3.3.6}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}Defining and Training for the Predictive GAN}{28}}
\newlabel{subsection:training_pred}{{3.3.7}{28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimentation Plan}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experimental Procedure}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Results}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Test results for untrained generators}}{33}}
\newlabel{table:before_training}{{4.1}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Test results for trained generators}}{33}}
\newlabel{table:after_training}{{4.2}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Average test results for trained generators}}{34}}
\newlabel{table:after_training_avg}{{4.3}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Result improvement from before to after training}}{34}}
\newlabel{table:training_change}{{4.4}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Average performance change across all experiments}}{34}}
\newlabel{table:avg_result}{{4.5}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation}{34}}
\newlabel{subsection:evaluation}{{4.4}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Visualization of the generator output as produced in the 9th discriminative training instance, before training. The first 40000 bits are presented as a 200 by 200 grid. The non-randomness of the output is obvious due to the visible patterns.}}{35}}
\newlabel{figure:visualize_discriminative_before}{{4.1}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Visualization of the generator output as produced in the 9th discriminative training instance, after 200000 training iterations. The first 40000 bits are presented as a 200 by 200 grid. Clearly the generator neural network has learned to produce more randomness, as no patterns can reasonably be discerned by a human observer.}}{36}}
\newlabel{figure:visualize_discriminative_after}{{4.2}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualization of the generator output as produced in the 9th predictive training instance, before training. The first 40000 bits are presented as a 200 by 200 grid. The non-randomness of the output is obvious due to the visible patterns.}}{36}}
\newlabel{figure:visualize_predictive_before}{{4.3}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of the generator output as produced in the 9th predictive training instance, after 200000 training iterations. The first 40000 bits are presented as a 200 by 200 grid. As with the discriminative training, no patterns can reasonably be discerned by a human observer; clearly the network has learned some notion of randomness.}}{37}}
\newlabel{figure:visualize_predictive_after}{{4.4}{37}}
\citation{barker2007recommendation}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{kelsey1998cryptanalytic}
\citation{deng2017developments}
\citation{russel2009artificial}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{38}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{5}{38}}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\citation{rukhin2001statistical}
\citation{anderson2010security}
\bibstyle{plain}
\bibdata{references.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Further Investigation}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Glossary}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Statistical Test Result Sample}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:sample}{{B}{42}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  0346D6B75A58D8EBC23E5F26668961A21A75FE06B90580A2747F093EDEF086C8.pygtex,
  B11CAF43CB2D515CCA0DD3BA15A156291A75FE06B90580A2747F093EDEF086C8.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8DD36A5E4086E63E105442262DEEA917C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BB7933F743825D655B75F5E42B9919DD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BB7933F743825D655B75F5E42B9919DD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  1337CB727A7C403D68A1CA7627DE47231A75FE06B90580A2747F093EDEF086C8.pygtex,
  D508C7FF36617C527AD0838A6E8D3DDA7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8DD36A5E4086E63E105442262DEEA917C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  52F77DE1933CD5A7334AA5FBFC6022CB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C6566F64461986FFE46C913E76644B707C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  6427AD6B4DAB1FF1309636D6DAAF42FB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  369990B384D3CB76276C5AA5C98725C97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  02F86E13CDE03EA48467EF4003E5EBE97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EDC6F24823F445DB0DA9CE539827FA557C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C8BBB379E618C1BD946023B78A4BF7BF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  14315537A739FB38DFB3645918E4E0037C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9687E97308B3EE6011878DF982BFA4727C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  470C1201BA4FAAB322FAC8FD6663615F7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EB844645E8E61DE0A4CF4B991E65E63E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  FC158F94C02041F0DE8136BBDAFF87F37C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA933CDAC9C8F936D1675308EED9A1017C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9CDB0EFE09A8295DBCAF1E387E06C7157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  684B83286E503589996B08CBEC11F9A57C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  02F86E13CDE03EA48467EF4003E5EBE97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  4CABC0EE807D64644CF35F9838EE66157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  A619A29DBD9DEBC781C0E120D6D85F207C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BFD9CE99FA621FCBBE7EEBB4CEE2D69E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C43A8BA2DF2E19D69806B510C5AABF6A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0EF06D23DD18D4D816DFE614E11C19D07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  961AE9C2A0310966E2AE9407EB738DB07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  A6385927A5BD165593D87E83595977E27C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  01A0ABC42B584A322669B1256A1BED317C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  646FFD27C53994357D09A9D584A0F2F77C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CEE7E034615BABC7CD600618AF3430CE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E57A574C89D18B89B1AD0A07EA0B6A811A75FE06B90580A2747F093EDEF086C8.pygtex,
  980BFFE1404D71BA492B4F81088798491A75FE06B90580A2747F093EDEF086C8.pygtex,
  AAE3C1BE63075E10642306F96B7BA7761A75FE06B90580A2747F093EDEF086C8.pygtex,
  8B5100D929E0C7BE8F6FBC2E0019876B7C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Summary of Supporting Material}{46}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
