\relax 
\providecommand{\transparent@use}[1]{}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{4}}
\citation{barker2007recommendation}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{donald1998art}
\citation{katz2014introduction}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{google2018automl}
\citation{amazon2018aws}
\citation{microsoft2018azure}
\citation{forbes2016short}
\citation{google2018trends}
\citation{google2018trends}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{goodfellow2014generative}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Search interest for ``deep learning" as a simple metric of public interest in machine learning. Data source: Google Trends (www.google.com/trends) \cite  {google2018trends}}}{6}}
\newlabel{figure:googletrends_dl}{{1.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims and Motivation}{6}}
\newlabel{subsection:aims}{{1.1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Approach, Scope, and Assumptions}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Findings and Contributions}{7}}
\citation{klimov2002analysis}
\citation{donald1998art}
\citation{bennett2009randomness}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Numbers Sequences and Generators}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Random Bits, Random Numbers, and Entropy}{8}}
\citation{rukhin2001statistical}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{barker2007recommendation}
\citation{cover2012elements}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A discrete uniform probability distribution for an arbitrary experiment with 10 possible outcomes.}}{9}}
\newlabel{figure:uniform_distribution}{{2.1}{9}}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{bierhorst2018experimentally}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\newlabel{eq:entropy}{{2.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random Number Generators}{10}}
\citation{ferguson2010cryptography}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{matsumoto1998mersenne}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{donald1998art}
\citation{ferguson2010cryptography}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A high-level view of the operation of a PRNG \cite  {kelsey1998cryptanalytic}.}}{11}}
\newlabel{figure:prng_high_level}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Pseudo-Random Number Generators}{11}}
\newlabel{subsection:prngs}{{2.1.3}{11}}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{kelsey1998cryptanalytic}
\citation{anderson2010security}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Random Numbers and Cryptography}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Cryptographic Applications of Random Numbers}{12}}
\@writefile{tdo}{\contentsline {todo}{find RFC}{12}}
\pgfsyspdfmark {pgfid1}{23554435}{28181045}
\pgfsyspdfmark {pgfid4}{36577345}{28195790}
\pgfsyspdfmark {pgfid5}{38002753}{27927093}
\@writefile{tdo}{\contentsline {todo}{MCTS}{12}}
\pgfsyspdfmark {pgfid6}{13407022}{26460733}
\pgfsyspdfmark {pgfid9}{36577345}{25666100}
\pgfsyspdfmark {pgfid10}{38002753}{25397403}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Requirements for Cryptographic Security}{12}}
\newlabel{subsection:crypto_requirements}{{2.2.2}{12}}
\@writefile{tdo}{\contentsline {todo}{definitions from page 171}{12}}
\pgfsyspdfmark {pgfid11}{4661699}{5142442}
\pgfsyspdfmark {pgfid14}{36577345}{5157187}
\pgfsyspdfmark {pgfid15}{38002753}{4888490}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\citation{terr2009math}
\citation{terr2009math}
\citation{rukhin2001statistical}
\citation{lavasani2009practical}
\citation{rukhin2001statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The probability distribution of a random variable representing the number of heads in a repeated fair coin-tossing experiment, which is equivalent to repeated random sampling from ${0, 1}$. Image from \cite  {terr2009math}.}}{13}}
\newlabel{figure:distribution}{{2.3}{13}}
\@writefile{tdo}{\contentsline {todo}{stuff from intro to modern cryptography, page 49 onwards}{13}}
\pgfsyspdfmark {pgfid16}{4661699}{32440671}
\pgfsyspdfmark {pgfid19}{36577345}{32455416}
\pgfsyspdfmark {pgfid20}{38002753}{32186719}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Testing Number Sequences for Randomness}{13}}
\newlabel{subsection:testing_prngs}{{2.2.3}{13}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introduction to Neural Networks}{14}}
\newlabel{subsection:neural_intro}{{2.3.1}{14}}
\newlabel{eq:activation}{{2.2}{14}}
\newlabel{eq:activation_vector}{{2.3}{14}}
\newlabel{eq:neural_net_composition}{{2.4}{14}}
\citation{russel2009artificial}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Schematic of the computation performed by a unit, drawn in accordance to the definitions given by Russel and Norvig \cite  [p.728]{russel2009artificial}. A weighted sum $in_j$ of the unit's inputs is computed, and the resulting value becomes the argument of the activation function $g$. The activation's output is the node's output. This schematic corresponds to equation 2.2\hbox {}.}}{15}}
\newlabel{figure:neural_unit}{{2.4}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Learning in Neural Networks}{15}}
\@writefile{tdo}{\contentsline {todo}{image of stable vs wiggly loss}{15}}
\pgfsyspdfmark {pgfid21}{27695636}{4002500}
\pgfsyspdfmark {pgfid24}{36577345}{4017245}
\pgfsyspdfmark {pgfid25}{38002753}{3748548}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{sharma2017activation}
\citation{sharma2017activation}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Activation Functions}{16}}
\newlabel{eq:relu}{{2.5}{16}}
\newlabel{eq:leakyrelu}{{2.6}{16}}
\citation{goodfellow2016deep}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Left: the ReLU function. Right: the LeakyReLU function. Note the zero gradient for negative inputs on the left, and how this problem is adressed on the right. Image courtesy of \cite  {sharma2017activation}.}}{17}}
\newlabel{figure:relu}{{2.5}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Simple example of the structure of a feedforward network. The network has two hidden layers, for a total depth of 4. The network's width is 6, as given by the size of the hidden layers. Image drawn using draw.io \cite  {jgraph2018draw}}}{17}}
\newlabel{figure:feedforward}{{2.6}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Fully Connected Feed-Forward Networks}{17}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{albelwi2017framework}
\citation{albelwi2017framework}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{goodfellow2016nips.}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Convolutional Neural Networks}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Generative Adversarial Networks}{18}}
\newlabel{subsection:generativeadversarial}{{2.3.6}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A representation of the connectivity between the units of a 1-dimensional input layer (white) and the filters in a convolutional layer (yellow), with each convolutional unit's input weights shown in the squares. In both images, the kernel size is 3; on the left the stride of the layer is 1, while on the right it is 2. Furthermore, the convolutional layer on the left has two filters. Note how both filters are connected to the input in the same way, and how each filter has its own set of input weights. The image is based on the CS231n lecture notes by Andrej Karpathy \cite  [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers]{karpathy2017cs231n} and was drawn with draw.io \cite  {jgraph2018draw}.}}{19}}
\newlabel{figure:convolution1d}{{2.7}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A convolutional layer preserves the dimensions of its input matrix, but produces an output with a larger depth dimensions depending on the number of filters. This is followed by pooling layers, which down-sample the data in the width and height dimensions. Image courtesy of \cite  {albelwi2017framework}.}}{19}}
\newlabel{figure:conv_pooling}{{2.8}{19}}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The conceptual structure of a generative adversarial network. The outputs of the generator are fed into the discriminator along with samples from the original dataset; the performance of the discriminator is factored into the loss function of the generator. Image drawn with draw.io \cite  {jgraph2018draw}.}}{20}}
\newlabel{figure:gan}{{2.9}{20}}
\newlabel{eq:gan_train}{{2.7}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Related Work}{20}}
\newlabel{section:related_work}{{2.4}{20}}
\citation{desai2012pseudo}
\citation{desai2011pseudo}
\citation{tirdad2010hopfield}
\citation{abadi2016learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Papers on Using Neural Networks as Pseudo-Random Number Generators}{21}}
\@writefile{tdo}{\contentsline {todo}{more detail because this is interesting}{21}}
\pgfsyspdfmark {pgfid26}{27493385}{30927537}
\pgfsyspdfmark {pgfid29}{36577345}{30942282}
\pgfsyspdfmark {pgfid30}{38002753}{30673585}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Learning to Protect Communications with Adversarial Neural Cryptography}{21}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design and Implementation}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Conceptual Design}{22}}
\newlabel{section:conceptual_design}{{3.1}{22}}
\newlabel{eq:conceptual_prng}{{3.1}{22}}
\citation{russel2009artificial}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The conceptual difference in the common implementation of PRNGs (top) and the implementation using GANs in this work (bottom). Image produced using draw.io \cite  {jgraph2018draw}.}}{23}}
\newlabel{figure:conceptual_difference}{{3.1}{23}}
\citation{karpathy2017cs231n}
\citation{kingma2014adam}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The two approaches differ at the conceptual level in the the discriminative approach (left) requires a source of true randomness which it attempts to emulate, while the predictive approach (right) is an even purer ``game" between the two networks, with no side inputs. Image produced using draw.io \cite  {jgraph2018draw}.}}{24}}
\newlabel{figure:approach_comparison}{{3.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of the generator. Each fully connected feed-forward layer's activation function (blue) is shown; the output layer (red) differs from the other layers in that it does not use the leaky ReLU activation, but rather a modulus function. Image produced using draw.io \cite  {jgraph2018draw}.}}{24}}
\newlabel{figure:architecture_generator}{{3.3}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Generative Model}{24}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{numpy}
\citation{numpy}
\citation{tensorflow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Convolutional discriminator architecture. The output of the generator is convolved multiple times in order to extract higher-level features from the sequence; this is followed by pooling to reduce the output size, and fully connected feed-forward layers to produce the final classification output. Image produced using draw.io \cite  {jgraph2018draw}.}}{25}}
\newlabel{figure:architecture_conv}{{3.4}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Convolutional predictor architecture. The design is shared with the discriminator, but the size of the input sequence and the meaning of the output scalar are different. The predictor produces a guess for the last value in the generator's output based on the previous values. Image produced using draw.io \cite  {jgraph2018draw}.}}{25}}
\newlabel{figure:architecture_conv}{{3.5}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Discriminative Model}{25}}
\citation{bhatia2017why}
\citation{tensorflow2018intro}
\citation{tensorflow2018intro}
\citation{tensorflow2018graphs}
\citation{tensorflow2018intro}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Predictive Model}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation Technologies}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to TensorFlow}{26}}
\citation{tensorflow2018tutorials}
\citation{tensorflow2018tfgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Supporting Technologies}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Software Design}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}System Parameterization}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Generative Model}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Discriminative and Predictive Models}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Custom Tensor Operations}{30}}
\newlabel{subsection:custom_ops}{{3.3.4}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Obtaining Training Data}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Defining and Training the Discriminative GAN}{31}}
\newlabel{subsection:training_disc}{{3.3.6}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}Defining and Training for the Predictive GAN}{32}}
\newlabel{subsection:training_pred}{{3.3.7}{32}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Procedure}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Training}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Statistical Testing}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Results}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Effect of Training Duration and Generator Dimensions}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces gooby}}{34}}
\newlabel{figure:discriminator_loss}{{4.1}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Aggregate test results for the generators before training}}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Aggregate test results for the generators after training}}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Illustrated improvements from before training to after training, for each training instance}}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Average performance change for the discriminative and predictive approaches across all tests}}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Visualization of the generator output as produced in the 9th discriminative training instance, before training. The first 40000 bits are presented as a 200 by 200 grid. The non-randomness of the output is obvious due to the visible patterns.}}{36}}
\newlabel{figure:visualize_discriminative_before}{{4.2}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Computational Complexity}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Evaluation}{36}}
\newlabel{subsection:evaluation}{{4.5}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Visualization of the generator output as produced in the 9th discriminative training instance, after 200000 training iterations. The first 40000 bits are presented as a 200 by 200 grid. Clearly the generator neural network has learned to produce more randomness, as no patterns can reasonably be discerned by a human observer.}}{37}}
\newlabel{figure:visualize_discriminative_after}{{4.3}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of the generator output as produced in the 9th predictive training instance, before training. The first 40000 bits are presented as a 200 by 200 grid. The non-randomness of the output is obvious due to the visible patterns.}}{38}}
\newlabel{figure:visualize_predictive_before}{{4.4}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Visualization of the generator output as produced in the 9th predictive training instance, after 200000 training iterations. The first 40000 bits are presented as a 200 by 200 grid. As with the discriminative training, no patterns can reasonably be discerned by a human observer; clearly the network has learned some notion of randomness.}}{39}}
\newlabel{figure:visualize_predictive_after}{{4.5}{39}}
\citation{barker2007recommendation}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{kelsey1998cryptanalytic}
\citation{deng2017developments}
\citation{russel2009artificial}
\citation{desai2011pseudo}
\citation{desai2012pseudo}
\citation{tirdad2010hopfield}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{5}{40}}
\bibstyle{plain}
\bibdata{references.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Further Investigation}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{bhatia2017why}{1}
\bibcite{numpy}{2}
\bibcite{forbes2016short}{3}
\bibcite{tensorflow}{4}
\bibcite{tensorflow2018graphs}{5}
\bibcite{tensorflow2018intro}{6}
\bibcite{tensorflow2018tutorials}{7}
\bibcite{abadi2016learning}{8}
\bibcite{albelwi2017framework}{9}
\bibcite{amazon2018aws}{10}
\bibcite{anderson2010security}{11}
\bibcite{barker2007recommendation}{12}
\bibcite{bennett2009randomness}{13}
\bibcite{bierhorst2018experimentally}{14}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{42}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{microsoft2018azure}{15}
\bibcite{cover2012elements}{16}
\bibcite{deng2017developments}{17}
\bibcite{desai2011pseudo}{18}
\bibcite{desai2012pseudo}{19}
\bibcite{donald1998art}{20}
\bibcite{ferguson2010cryptography}{21}
\bibcite{goodfellow2016deep}{22}
\bibcite{goodfellow2014generative}{23}
\bibcite{karpathy2017cs231n}{24}
\bibcite{katz2014introduction}{25}
\bibcite{kelsey1998cryptanalytic}{26}
\bibcite{kingma2014adam}{27}
\bibcite{klimov2002analysis}{28}
\bibcite{lavasani2009practical}{29}
\bibcite{google2018automl}{30}
\bibcite{google2018trends}{31}
\bibcite{jgraph2018draw}{32}
\bibcite{matsumoto1998mersenne}{33}
\bibcite{menezes1996handbook}{34}
\bibcite{rukhin2001statistical}{35}
\bibcite{russel2009artificial}{36}
\bibcite{sharma2017activation}{37}
\bibcite{tensorflow2018tfgan}{38}
\bibcite{terr2009math}{39}
\bibcite{tirdad2010hopfield}{40}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Glossary}{45}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Statistical Test Result Sample}{46}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  0346D6B75A58D8EBC23E5F26668961A21A75FE06B90580A2747F093EDEF086C8.pygtex,
  B11CAF43CB2D515CCA0DD3BA15A156291A75FE06B90580A2747F093EDEF086C8.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  1337CB727A7C403D68A1CA7627DE47231A75FE06B90580A2747F093EDEF086C8.pygtex,
  D508C7FF36617C527AD0838A6E8D3DDA7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8DD36A5E4086E63E105442262DEEA917C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  52F77DE1933CD5A7334AA5FBFC6022CB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C6566F64461986FFE46C913E76644B707C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  6427AD6B4DAB1FF1309636D6DAAF42FB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  369990B384D3CB76276C5AA5C98725C97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  02F86E13CDE03EA48467EF4003E5EBE97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EDC6F24823F445DB0DA9CE539827FA557C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C8BBB379E618C1BD946023B78A4BF7BF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  14315537A739FB38DFB3645918E4E0037C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9687E97308B3EE6011878DF982BFA4727C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  470C1201BA4FAAB322FAC8FD6663615F7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EB844645E8E61DE0A4CF4B991E65E63E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  FC158F94C02041F0DE8136BBDAFF87F37C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA933CDAC9C8F936D1675308EED9A1017C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0B816804D4241E6C50E1DBEDD37F24D47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9CDB0EFE09A8295DBCAF1E387E06C7157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  684B83286E503589996B08CBEC11F9A57C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  02F86E13CDE03EA48467EF4003E5EBE97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  4CABC0EE807D64644CF35F9838EE66157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  A619A29DBD9DEBC781C0E120D6D85F207C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BFD9CE99FA621FCBBE7EEBB4CEE2D69E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  C43A8BA2DF2E19D69806B510C5AABF6A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0EF06D23DD18D4D816DFE614E11C19D07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  961AE9C2A0310966E2AE9407EB738DB07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  A6385927A5BD165593D87E83595977E27C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  01A0ABC42B584A322669B1256A1BED317C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E57A574C89D18B89B1AD0A07EA0B6A811A75FE06B90580A2747F093EDEF086C8.pygtex,
  980BFFE1404D71BA492B4F81088798491A75FE06B90580A2747F093EDEF086C8.pygtex,
  AAE3C1BE63075E10642306F96B7BA7761A75FE06B90580A2747F093EDEF086C8.pygtex,
  8B5100D929E0C7BE8F6FBC2E0019876B7C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Summary of Supporting Material}{50}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
