\relax 
\citation{russel2009artificial}
\citation{google2018automl}
\citation{amazon2018aws}
\citation{microsoft2018azure}
\citation{forbes2016short}
\citation{google2018trends}
\citation{google2018trends}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{stuff about security}{4}}
\pgfsyspdfmark {pgfid1}{4661699}{31370997}
\pgfsyspdfmark {pgfid4}{36577345}{31385742}
\pgfsyspdfmark {pgfid5}{38002753}{31117045}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Search interest for "deep learning" as an example of interest in machine learning. Data source: Google Trends (www.google.com/trends) \cite  {google2018trends}}}{4}}
\newlabel{figure:googletrends_dl}{{1.1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Aims}{5}}
\newlabel{subsection:aims}{{1.2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Report Structure}{5}}
\citation{klimov2002analysis}
\citation{donald1998art}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Numbers Sequences and Generators}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Random Bits, Random Numbers, and Entropy}{6}}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{cover2012elements}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{deng2017developments}
\citation{anderson2010security}
\newlabel{eq:entropy}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Applications of Random Numbers}{7}}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\@writefile{tdo}{\contentsline {todo}{find RFC}{8}}
\pgfsyspdfmark {pgfid6}{23554435}{46548023}
\pgfsyspdfmark {pgfid9}{36577345}{46562768}
\pgfsyspdfmark {pgfid10}{38002753}{46294071}
\@writefile{tdo}{\contentsline {todo}{MCTS}{8}}
\pgfsyspdfmark {pgfid11}{13407022}{44827711}
\pgfsyspdfmark {pgfid14}{36577345}{44033078}
\pgfsyspdfmark {pgfid15}{38002753}{43764381}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Random Number Generators}{8}}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A high-level view of the operation of a PRNG \cite  {kelsey1998cryptanalytic}.}}{9}}
\newlabel{figure:prng_high_level}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Pseudo-Random Number Generators}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Cryptographic Requirements}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Testing Pseudo-Random Sequences}{9}}
\citation{rukhin2001statistical}
\citation{terr2009math}
\citation{terr2009math}
\citation{rukhin2001statistical}
\citation{lavasani2009practical}
\citation{rukhin2001statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The probability distribution of a random variable representing the number of heads in a repeated fair coin-tossing experiment, which is equivalent to repeated random sampling from ${0, 1}$. Image from \cite  {terr2009math}.}}{10}}
\newlabel{figure:distribution}{{2.2}{10}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{russel2009artificial}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction to Neural Networks}{11}}
\newlabel{subsection:neural_intro}{{2.2.1}{11}}
\newlabel{eq:weighted_sum}{{2.2}{11}}
\newlabel{eq:activation}{{2.3}{11}}
\newlabel{eq:neural_net_composition}{{2.4}{11}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{karpathy2017cs231n}
\citation{[Optimization: Stochastic Gradient Descent] karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of the computation performed by a unit, from Russel and Norvig \cite  [p.728]{russel2009artificial}. This schematic corresponds to equation 2.3\hbox {}.}}{12}}
\newlabel{figure:neural_unit}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Learning in Neural Networks}{12}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Activation Functions}{13}}
\newlabel{eq:relu}{{2.5}{13}}
\citation{karpathy2017cs231n}
\citation{sharma2017activation}
\citation{sharma2017activation}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: the ReLU function. Right: the LeakyReLU function. Image courtesy of \cite  {sharma2017activation}.}}{14}}
\newlabel{figure:relu}{{2.4}{14}}
\newlabel{eq:leakyrelu}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Feed-Forward Networks}{14}}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple example of the structure of a feedforward network, from the CS231n lecture notes by Andrej Karpathy \cite  [Neural Networks Part 1 : Setting up the Architecture]{karpathy2017cs231n}}}{15}}
\newlabel{figure:feedforward}{{2.5}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Convolutional Neural Networks}{15}}
\citation{karpathy2017cs231n}
\citation{albelwi2017framework}
\citation{albelwi2017framework}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A representation of the connectivity between the units of a 1-dimensional input layer (white) and a single filter in a convolutional layer (yellow), with each neuron's weights in the upper right corner. In both images, each unit in the convolutional layer has a receptive field (or kernel size) of 3. The inputs have a width of 5, with zero-padding of 1. The two images demonstrate different strides for the convolutional layer (stride 1 on the left, stride 2 on the right). Image courtesy of the CS231n lecture notes by Andrej Karpathy \cite  [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers]{karpathy2017cs231n}.}}{16}}
\newlabel{figure:convolution1d}{{2.6}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Recurrent Neural Networks}{16}}
\newlabel{eq:dynamic_system}{{2.7}{16}}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A convolutional layer preserves the dimensions of its input matrix, but produces an output with a larger depth dimensions depending on the number of filters. This is followed by pooling layers, which down-sample the data in the width and height dimensions. Image courtesy of \cite  {albelwi2017framework}.}}{17}}
\newlabel{figure:conv_pooling}{{2.7}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Unfolding a recurrent network without outputs. The black square represents a delay of one timestep in the application of previous inputs to the computation. Image courtesy of \cite  [p. 370]{goodfellow2016deep}.}}{17}}
\newlabel{figure:conv_pooling}{{2.8}{17}}
\newlabel{eq:dynamic_system}{{2.8}{17}}
\newlabel{eq:recurrent_net}{{2.9}{17}}
\citation{goodfellow2014generative}
\citation{goodfellow2016nips}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Generative Adversarial Networks}{18}}
\newlabel{eq:gan_train}{{2.10}{18}}
\citation{abadi2016learning}
\citation{desai2012pseudo}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Work}{19}}
\newlabel{section:related_work}{{2.3}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Learning to Protect Communications with Adversarial Neural Cryptography}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Papers on Using Neural Networks as Pseudo-Random Number Generators}{19}}
\citation{desai2011pseudo}
\citation{tirdad2010hopfield}
\citation{melicher2016fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks}{20}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design and Implementation}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Conceptual Design}{21}}
\newlabel{section:conceptual_design}{{3.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Discriminative Approach}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Predictive Approach}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Generative Model}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Discriminative and Predictive Models}{21}}
\citation{tensorflow}
\citation{keras}
\citation{numpy}
\citation{keras}
\@writefile{toc}{\contentsline {subsubsection}{Convolutional Architecture}{22}}
\@writefile{toc}{\contentsline {subsubsection}{LSTM Architecture}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Mixed Convolutional and LSTM Architecture}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Hyperparameters and Training Parameters}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation Technologies}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Python}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}TensorFlow}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Keras}{22}}
\citation{keras}
\citation{keras}
\citation{ricard2017generative}
\citation{atienza2017gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Software Libraries}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Supporting Tools}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Software Design}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Generative Model}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Discriminative and Predictive Models}{24}}
\citation{keras}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Connecting the Models Adversarially}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Obtaining Training Data}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Training Procedure for the Discriminative GAN}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Training Procedure for the Predictive GAN}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}TensorFlow Functions}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.8}Utilities}{26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Discriminative Training}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Training Parameters}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Results}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Analysis}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Predictive Training}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Training Parameters}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Results}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Analysis}{27}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{5}{28}}
\bibstyle{plain}
\bibdata{references.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Further Investigation}{29}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{atienza2017gan}{1}
\bibcite{ricard2017generative}{2}
\bibcite{keras}{3}
\bibcite{numpy}{4}
\bibcite{forbes2016short}{5}
\bibcite{tensorflow}{6}
\bibcite{abadi2016learning}{7}
\bibcite{albelwi2017framework}{8}
\bibcite{amazon2018aws}{9}
\bibcite{anderson2010security}{10}
\bibcite{barker2007recommendation}{11}
\bibcite{cormen2009introduction}{12}
\bibcite{microsoft2018azure}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{cover2012elements}{14}
\bibcite{deng2017developments}{15}
\bibcite{desai2011pseudo}{16}
\bibcite{desai2012pseudo}{17}
\bibcite{donald1998art}{18}
\bibcite{ferguson2010cryptography}{19}
\bibcite{goodfellow2016nips}{20}
\bibcite{goodfellow2016deep}{21}
\bibcite{goodfellow2014generative}{22}
\bibcite{karpathy2017cs231n}{23}
\bibcite{kelsey1998cryptanalytic}{24}
\bibcite{klimov2002analysis}{25}
\bibcite{lavasani2009practical}{26}
\bibcite{google2018automl}{27}
\bibcite{google2018trends}{28}
\bibcite{melicher2016fast}{29}
\bibcite{menezes1996handbook}{30}
\bibcite{rukhin2001statistical}{31}
\bibcite{russel2009artificial}{32}
\bibcite{sharma2017activation}{33}
\bibcite{terr2009math}{34}
\bibcite{tirdad2010hopfield}{35}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Training Data}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Code Listings}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Generator}{34}}
\newlabel{appendix:generator}{{B.1}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Adversary, Convolutional Architecture}{34}}
\newlabel{appendix:convolutional}{{B.2}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Adversary, LSTM Architecture}{35}}
\newlabel{appendix:lstm}{{B.3}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Adversary, Convolutional LSTM Architecture}{35}}
\newlabel{appendix:convlstm}{{B.4}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Training of Discriminative GAN}{36}}
\newlabel{appendix:training_discgan}{{B.5}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Training of Predictive GAN}{36}}
\newlabel{appendix:training_predgan}{{B.6}{36}}
\citation{cormen2009introduction}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Mathematical Notation}{38}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:mathematics}{{C}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Set Theory}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Probability Theory}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Linear Algebra}{38}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  0346D6B75A58D8EBC23E5F26668961A27C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F478FD3E27D1A397DBBF491DFD095CAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  6FE7A95D980E74F7D76BEBADB1CC053D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  667D234BAA4D209AA4B8A64DFD2C69CA7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0F5CEFA8370FF2FAFEF10B5CF5B2C8D37C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  470C1201BA4FAAB322FAC8FD6663615F7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  96124A8E093CCC28031C676CC8AB83137C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  D0A92E9AE8085D509B5902092E3BAFA07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  742E972295C34C97622DDA3852C1CFAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  2F95209C0EB47CDE4C0FB4BFDB8897377C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  72FE05AE8E57F8A28046743ADEDC78ED7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F1C84F8167B4ED8A289218AC27A768DD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  3815CAAAF6D678434BB0DD353CA1D8487C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BC4E70DAC4A264E62A80B58427011B8D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F82D86802EB00649D1302D4B2AD0C11A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  090F3DDE3F4827A496BE04C4299DC1DF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  4CABC0EE807D64644CF35F9838EE66157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  A33D1773947B2A2B3C0EB4CFB4E862937C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  7BB2296F2C23E3D8D42263634613906A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CC8F4CB50C34B9656FB22C920A967C4C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  38E5D2095B76575F55EB2D505DD5B4417C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  B721F2FD0C2484502B723AAEA86A00067C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  793EE9ECA4E3E41EA42F6648FE4D79027C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9914B55C912FBA0669AE2AD56336B4C77C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EBD91A97485EB6386993AB64D42487FE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8B38AF19D2579281620EB8F04349BFC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  5164D5E84F853507392851B55CBD98F77C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Neural Networks}{39}}
\newlabel{eq:sigmoid}{{C.1}{39}}
\newlabel{eq:tanh}{{C.2}{39}}
