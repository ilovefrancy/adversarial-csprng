\relax 
\citation{russel2009artificial}
\citation{google2018automl}
\citation{amazon2018aws}
\citation{microsoft2018azure}
\citation{forbes2016short}
\citation{google2018trends}
\citation{google2018trends}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{stuff about security}{4}}
\pgfsyspdfmark {pgfid1}{4661699}{31370997}
\pgfsyspdfmark {pgfid4}{36577345}{31385742}
\pgfsyspdfmark {pgfid5}{38002753}{31117045}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Search interest for "deep learning" as an example of interest in machine learning. Data source: Google Trends (www.google.com/trends) \cite  {google2018trends}}}{4}}
\newlabel{figure:googletrends_dl}{{1.1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Aims}{5}}
\newlabel{subsection:aims}{{1.2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Report Structure}{5}}
\citation{klimov2002analysis}
\citation{donald1998art}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Numbers Sequences and Generators}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Random Bits, Random Numbers, and Entropy}{6}}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{cover2012elements}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{deng2017developments}
\citation{anderson2010security}
\newlabel{eq:entropy}{{2.1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Applications of Random Numbers}{7}}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\@writefile{tdo}{\contentsline {todo}{find RFC}{8}}
\pgfsyspdfmark {pgfid6}{23554435}{46548023}
\pgfsyspdfmark {pgfid9}{36577345}{46562768}
\pgfsyspdfmark {pgfid10}{38002753}{46294071}
\@writefile{tdo}{\contentsline {todo}{MCTS}{8}}
\pgfsyspdfmark {pgfid11}{13407022}{44827711}
\pgfsyspdfmark {pgfid14}{36577345}{44033078}
\pgfsyspdfmark {pgfid15}{38002753}{43764381}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Random Number Generators}{8}}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A high-level view of the operation of a PRNG \cite  {kelsey1998cryptanalytic}.}}{9}}
\newlabel{figure:prng_high_level}{{2.1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Pseudo-Random Number Generators}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Cryptographic Randomness Requirements}{9}}
\newlabel{subsection:crypto_requirements}{{2.1.5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Testing Pseudo-Random Sequences}{9}}
\newlabel{subsection:testing_prngs}{{2.1.6}{9}}
\citation{rukhin2001statistical}
\citation{terr2009math}
\citation{terr2009math}
\citation{rukhin2001statistical}
\citation{lavasani2009practical}
\citation{rukhin2001statistical}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The probability distribution of a random variable representing the number of heads in a repeated fair coin-tossing experiment, which is equivalent to repeated random sampling from ${0, 1}$. Image from \cite  {terr2009math}.}}{10}}
\newlabel{figure:distribution}{{2.2}{10}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{russel2009artificial}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Artificial Neural Networks}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Introduction to Neural Networks}{11}}
\newlabel{subsection:neural_intro}{{2.2.1}{11}}
\newlabel{eq:weighted_sum}{{2.2}{11}}
\newlabel{eq:activation}{{2.3}{11}}
\newlabel{eq:neural_net_composition}{{2.4}{11}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{karpathy2017cs231n}
\citation{[Optimization: Stochastic Gradient Descent] karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of the computation performed by a unit, from Russel and Norvig \cite  [p.728]{russel2009artificial}. This schematic corresponds to equation 2.3\hbox {}.}}{12}}
\newlabel{figure:neural_unit}{{2.3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Learning in Neural Networks}{12}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Activation Functions}{13}}
\newlabel{eq:relu}{{2.5}{13}}
\citation{karpathy2017cs231n}
\citation{sharma2017activation}
\citation{sharma2017activation}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: the ReLU function. Right: the LeakyReLU function. Image courtesy of \cite  {sharma2017activation}.}}{14}}
\newlabel{figure:relu}{{2.4}{14}}
\newlabel{eq:leakyrelu}{{2.6}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Feed-Forward Networks}{14}}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple example of the structure of a feedforward network, from the CS231n lecture notes by Andrej Karpathy \cite  [Neural Networks Part 1 : Setting up the Architecture]{karpathy2017cs231n}}}{15}}
\newlabel{figure:feedforward}{{2.5}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Convolutional Neural Networks}{15}}
\citation{karpathy2017cs231n}
\citation{albelwi2017framework}
\citation{albelwi2017framework}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A representation of the connectivity between the units of a 1-dimensional input layer (white) and a single filter in a convolutional layer (yellow), with each neuron's weights in the upper right corner. In both images, each unit in the convolutional layer has a receptive field (or kernel size) of 3. The inputs have a width of 5, with zero-padding of 1. The two images demonstrate different strides for the convolutional layer (stride 1 on the left, stride 2 on the right). Image courtesy of the CS231n lecture notes by Andrej Karpathy \cite  [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers]{karpathy2017cs231n}.}}{16}}
\newlabel{figure:convolution1d}{{2.6}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Recurrent Neural Networks}{16}}
\newlabel{eq:dynamic_system}{{2.7}{16}}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A convolutional layer preserves the dimensions of its input matrix, but produces an output with a larger depth dimensions depending on the number of filters. This is followed by pooling layers, which down-sample the data in the width and height dimensions. Image courtesy of \cite  {albelwi2017framework}.}}{17}}
\newlabel{figure:conv_pooling}{{2.7}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Unfolding a recurrent network without outputs. The black square represents a delay of one timestep in the application of previous inputs to the computation. Image courtesy of \cite  [p. 370]{goodfellow2016deep}.}}{17}}
\newlabel{figure:conv_pooling}{{2.8}{17}}
\newlabel{eq:dynamic_system}{{2.8}{17}}
\newlabel{eq:recurrent_net}{{2.9}{17}}
\citation{goodfellow2014generative}
\citation{goodfellow2016nips}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Generative Adversarial Networks}{18}}
\newlabel{subsection:generativeadversarial}{{2.2.7}{18}}
\newlabel{eq:gan_train}{{2.10}{18}}
\citation{abadi2016learning}
\citation{desai2012pseudo}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Related Work}{19}}
\newlabel{section:related_work}{{2.3}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Learning to Protect Communications with Adversarial Neural Cryptography}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Papers on Using Neural Networks as Pseudo-Random Number Generators}{19}}
\citation{desai2011pseudo}
\citation{tirdad2010hopfield}
\citation{melicher2016fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks}{20}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design and Implementation}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Conceptual Design}{21}}
\newlabel{section:conceptual_design}{{3.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Generative Model}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Discriminative and Predictive Models}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Evaluation and Randomness Requirements}{21}}
\newlabel{subsection:eval_and_randomness}{{3.1.3}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Architecture of the generator. Each fully connected feed-forward (FCFF) neural network layer is implemented in Keras using a \mintinline {python}{Dense} layer followed by a \mintinline {python}{LeakyReLU} layer. The network's inputs (two real numbers) and outputs (a sequence of real numbers of length $n$) are also shown. Image produced using draw.io \cite  {jgraph2018draw}.}}{22}}
\newlabel{figure:architecture_generator}{{3.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convolutional opponent architecture. \mintinline {python}{Reshape} and \mintinline {python}{Flatten} layers are required in Keras as the \mintinline {python}{Conv1D} takes a two-dimensional input. Image produced using draw.io \cite  {jgraph2018draw}.}}{22}}
\newlabel{figure:architecture_conv}{{3.2}{22}}
\citation{numpy}
\citation{numpy}
\citation{tensorflow}
\citation{bhatia2017why}
\citation{tensorflow2018intro}
\citation{tensorflow2018intro}
\citation{tensorflow2018graphs}
\citation{tensorflow2018intro}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation Technologies}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to TensorFlow}{23}}
\citation{ricard2017generative}
\citation{atienza2017gan}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Software Design}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Generative Model}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Discriminative and Predictive Models}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Connecting the Models Adversarially}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Obtaining Training Data}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Training Procedure for the Discriminative GAN}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Training Procedure for the Predictive GAN}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}Custom Tensor Operations}{26}}
\newlabel{subsection:custom_ops}{{3.3.7}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.8}Utilities}{26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{28}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Training and Evaluation Procedure}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Training and Evaluation Parameters}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Neural Network Hyperparameters}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Results}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{5}{30}}
\bibstyle{plain}
\bibdata{references.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Further Investigation}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{atienza2017gan}{1}
\bibcite{ricard2017generative}{2}
\bibcite{bhatia2017why}{3}
\bibcite{numpy}{4}
\bibcite{forbes2016short}{5}
\bibcite{tensorflow}{6}
\bibcite{tensorflow2018graphs}{7}
\bibcite{tensorflow2018intro}{8}
\bibcite{abadi2016learning}{9}
\bibcite{albelwi2017framework}{10}
\bibcite{amazon2018aws}{11}
\bibcite{anderson2010security}{12}
\bibcite{barker2007recommendation}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{cormen2009introduction}{14}
\bibcite{microsoft2018azure}{15}
\bibcite{cover2012elements}{16}
\bibcite{deng2017developments}{17}
\bibcite{desai2011pseudo}{18}
\bibcite{desai2012pseudo}{19}
\bibcite{donald1998art}{20}
\bibcite{ferguson2010cryptography}{21}
\bibcite{goodfellow2016nips}{22}
\bibcite{goodfellow2016deep}{23}
\bibcite{goodfellow2014generative}{24}
\bibcite{karpathy2017cs231n}{25}
\bibcite{kelsey1998cryptanalytic}{26}
\bibcite{klimov2002analysis}{27}
\bibcite{lavasani2009practical}{28}
\bibcite{google2018automl}{29}
\bibcite{google2018trends}{30}
\bibcite{jgraph2018draw}{31}
\bibcite{melicher2016fast}{32}
\bibcite{menezes1996handbook}{33}
\bibcite{rukhin2001statistical}{34}
\bibcite{russel2009artificial}{35}
\bibcite{sharma2017activation}{36}
\bibcite{terr2009math}{37}
\bibcite{tirdad2010hopfield}{38}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Training Data}{35}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Code Listings}{36}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Generator}{36}}
\newlabel{appendix:generator}{{B.1}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Adversary, Convolutional Architecture}{36}}
\newlabel{appendix:convolutional}{{B.2}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Adversary, LSTM Architecture}{37}}
\newlabel{appendix:lstm}{{B.3}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Adversary, Convolutional LSTM Architecture}{37}}
\newlabel{appendix:convlstm}{{B.4}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Training of Discriminative GAN}{38}}
\newlabel{appendix:training_discgan}{{B.5}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Training of Predictive GAN}{38}}
\newlabel{appendix:training_predgan}{{B.6}{38}}
\citation{cormen2009introduction}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Mathematical Notation}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendix:mathematics}{{C}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {C.1}Set Theory}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {C.2}Probability Theory}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {C.3}Linear Algebra}{40}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  0346D6B75A58D8EBC23E5F26668961A27C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F478FD3E27D1A397DBBF491DFD095CAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0A7A7FC8FCEE4E12836EB41F237EFD597C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  1ED4843BE0C5988D61C17DE14C3FBA1D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  0A7A7FC8FCEE4E12836EB41F237EFD597C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  1ED4843BE0C5988D61C17DE14C3FBA1D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  742E972295C34C97622DDA3852C1CFAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  D0A92E9AE8085D509B5902092E3BAFA07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  8AAE1D82DD56C249C0E7A7C02C0960AE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  742E972295C34C97622DDA3852C1CFAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  D0A92E9AE8085D509B5902092E3BAFA07C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  8AAE1D82DD56C249C0E7A7C02C0960AE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9687E97308B3EE6011878DF982BFA4727C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  470C1201BA4FAAB322FAC8FD6663615F7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EB844645E8E61DE0A4CF4B991E65E63E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  FC158F94C02041F0DE8136BBDAFF87F37C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA933CDAC9C8F936D1675308EED9A1017C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9CDB0EFE09A8295DBCAF1E387E06C7157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  684B83286E503589996B08CBEC11F9A57C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F1C84F8167B4ED8A289218AC27A768DD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  3815CAAAF6D678434BB0DD353CA1D8487C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BC4E70DAC4A264E62A80B58427011B8D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F82D86802EB00649D1302D4B2AD0C11A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  090F3DDE3F4827A496BE04C4299DC1DF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  4CABC0EE807D64644CF35F9838EE66157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  7BB2296F2C23E3D8D42263634613906A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CC8F4CB50C34B9656FB22C920A967C4C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  38E5D2095B76575F55EB2D505DD5B4417C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  977C575EF57AA823F92F25A2D4C8E60B7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  B93AECBB216C0B6DFB7774B39A0C70EF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  8A6C74665C9A6CB90F9EF54ECC93AF767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  3EDFF26758793F47CCD2986B898FB2A17C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  414D83DCFA26D6CFE6473DFD93D61EC47C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  68B329DA9893E34099C7D8AD5CB9C9407C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  68B329DA9893E34099C7D8AD5CB9C9407C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  B721F2FD0C2484502B723AAEA86A00067C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  793EE9ECA4E3E41EA42F6648FE4D79027C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9914B55C912FBA0669AE2AD56336B4C77C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EBD91A97485EB6386993AB64D42487FE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8B38AF19D2579281620EB8F04349BFC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  5164D5E84F853507392851B55CBD98F77C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
\@writefile{toc}{\contentsline {section}{\numberline {C.4}Neural Networks}{41}}
\newlabel{eq:sigmoid}{{C.1}{41}}
\newlabel{eq:tanh}{{C.2}{41}}
