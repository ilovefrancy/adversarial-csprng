\relax 
\citation{barker2007recommendation}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{menezes1996handbook}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{donald1998art}
\citation{katz2014introduction}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{google2018automl}
\citation{amazon2018aws}
\citation{microsoft2018azure}
\citation{forbes2016short}
\citation{google2018trends}
\citation{google2018trends}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Search interest for ``deep learning" as a simple metric of public interest in machine learning. Data source: Google Trends (www.google.com/trends) \cite  {google2018trends}}}{5}}
\newlabel{figure:googletrends_dl}{{1.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims and Motivation}{5}}
\newlabel{subsection:aims}{{1.1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Approach, Scope, and Assumptions}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Findings and Contributions}{6}}
\citation{klimov2002analysis}
\citation{donald1998art}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:background}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Random Numbers Sequences and Generators}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Random Bits, Random Numbers, and Entropy}{7}}
\citation{barker2007recommendation}
\citation{rukhin2001statistical}
\citation{menezes1996handbook}
\citation{cover2012elements}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\newlabel{eq:entropy}{{2.1}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Random Number Generators}{8}}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{ferguson2010cryptography}
\citation{menezes1996handbook}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\citation{rukhin2001statistical}
\citation{kelsey1998cryptanalytic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Pseudo-Random Number Generators}{9}}
\newlabel{subsection:prngs}{{2.1.3}{9}}
\@writefile{tdo}{\contentsline {todo}{stuff about how PRNG state changes based on previous outputs}{9}}
\pgfsyspdfmark {pgfid1}{4661699}{8010156}
\pgfsyspdfmark {pgfid4}{36577345}{8024901}
\pgfsyspdfmark {pgfid5}{38002753}{7756204}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A high-level view of the operation of a PRNG \cite  {kelsey1998cryptanalytic}.}}{10}}
\newlabel{figure:prng_high_level}{{2.1}{10}}
\citation{ferguson2010cryptography}
\citation{deng2017developments}
\citation{anderson2010security}
\citation{rukhin2001statistical}
\citation{rukhin2001statistical}
\citation{terr2009math}
\citation{terr2009math}
\citation{rukhin2001statistical}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Random Numbers and Cryptography}{11}}
\@writefile{tdo}{\contentsline {todo}{find RFC}{11}}
\pgfsyspdfmark {pgfid6}{23554435}{37260496}
\pgfsyspdfmark {pgfid9}{36577345}{37275241}
\pgfsyspdfmark {pgfid10}{38002753}{37006544}
\@writefile{tdo}{\contentsline {todo}{MCTS}{11}}
\pgfsyspdfmark {pgfid11}{13407022}{35540184}
\pgfsyspdfmark {pgfid14}{36577345}{34745551}
\pgfsyspdfmark {pgfid15}{38002753}{34476854}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Cryptographic Applications of Random Numbers}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Requirements for Cryptographic Security}{11}}
\newlabel{subsection:crypto_requirements}{{2.2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Testing Number Sequences for Randomness}{11}}
\newlabel{subsection:testing_prngs}{{2.2.3}{11}}
\citation{lavasani2009practical}
\citation{rukhin2001statistical}
\citation{russel2009artificial}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The probability distribution of a random variable representing the number of heads in a repeated fair coin-tossing experiment, which is equivalent to repeated random sampling from ${0, 1}$. Image from \cite  {terr2009math}.}}{12}}
\newlabel{figure:distribution}{{2.2}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Introduction to Neural Networks}{12}}
\newlabel{subsection:neural_intro}{{2.3.1}{12}}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{russel2009artificial}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of the computation performed by a unit, drawn in accordance to the definitions given by Russel and Norvig \cite  [p.728]{russel2009artificial}. A weighted sum $in_j$ of the unit's inputs is computed, and the resulting value becomes the argument of the activation function $g$. The activation's output is the node's output. This schematic corresponds to equation 2.2\hbox {}.}}{13}}
\newlabel{figure:neural_unit}{{2.3}{13}}
\newlabel{eq:activation}{{2.2}{13}}
\newlabel{eq:activation_vector}{{2.3}{13}}
\newlabel{eq:neural_net_composition}{{2.4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Learning in Neural Networks}{13}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Activation Functions}{14}}
\newlabel{eq:relu}{{2.5}{14}}
\citation{karpathy2017cs231n}
\citation{sharma2017activation}
\citation{sharma2017activation}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Left: the ReLU function. Right: the LeakyReLU function. Note the zero gradient for negative inputs on the left, and how this problem is adressed on the right. Image courtesy of \cite  {sharma2017activation}.}}{15}}
\newlabel{figure:relu}{{2.4}{15}}
\newlabel{eq:leakyrelu}{{2.6}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Feed-Forward Networks}{15}}
\citation{goodfellow2016deep}
\citation{goodfellow2016deep}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Simple example of the structure of a feedforward network, from the CS231n lecture notes by Andrej Karpathy \cite  [Neural Networks Part 1 : Setting up the Architecture]{karpathy2017cs231n}}}{16}}
\newlabel{figure:feedforward}{{2.5}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces A representation of the connectivity between the units of a 1-dimensional input layer (white) and a single filter in a convolutional layer (yellow), with each neuron's weights in the upper right corner. In both images, each unit in the convolutional layer has a receptive field (or kernel size) of 3. The inputs have a width of 5, with zero-padding of 1. The two images demonstrate different strides for the convolutional layer (stride 1 on the left, stride 2 on the right). Image courtesy of the CS231n lecture notes by Andrej Karpathy \cite  [Convolutional Neural Networks: Architectures, Convolution / Pooling Layers]{karpathy2017cs231n}.}}{16}}
\newlabel{figure:convolution1d}{{2.6}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Convolutional Neural Networks}{16}}
\citation{karpathy2017cs231n}
\citation{karpathy2017cs231n}
\citation{albelwi2017framework}
\citation{albelwi2017framework}
\citation{goodfellow2014generative}
\citation{goodfellow2016nips}
\citation{goodfellow2014generative}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Generative Adversarial Networks}{17}}
\newlabel{subsection:generativeadversarial}{{2.3.6}{17}}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces A convolutional layer preserves the dimensions of its input matrix, but produces an output with a larger depth dimensions depending on the number of filters. This is followed by pooling layers, which down-sample the data in the width and height dimensions. Image courtesy of \cite  {albelwi2017framework}.}}{18}}
\newlabel{figure:conv_pooling}{{2.7}{18}}
\newlabel{eq:gan_train}{{2.7}{18}}
\citation{abadi2016learning}
\citation{desai2012pseudo}
\citation{desai2011pseudo}
\citation{tirdad2010hopfield}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Related Work}{19}}
\newlabel{section:related_work}{{2.4}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Learning to Protect Communications with Adversarial Neural Cryptography}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Papers on Using Neural Networks as Pseudo-Random Number Generators}{19}}
\citation{melicher2016fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Fast, Lean, and Accurate: Modeling Password Guessability Using Neural Networks}{20}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Design and Implementation}{21}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:design}{{3}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Conceptual Design}{21}}
\newlabel{section:conceptual_design}{{3.1}{21}}
\newlabel{eq:conceptual_prng}{{3.1}{21}}
\citation{russel2009artificial}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The conceptual difference in the common implementation of PRNGs (top) and the implementation using GANs in this work (bottom). Image produced using draw.io \cite  {jgraph2018draw}.}}{22}}
\newlabel{figure:conceptual_difference}{{3.1}{22}}
\citation{karpathy2017cs231n}
\citation{kingma2014adam}
\citation{karpathy2017cs231n}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The two approaches differ at the conceptual level in the the discriminative approach (left) requires a source of true randomness which it attempts to emulate, while the predictive approach (right) is an even purer ``game" between the two networks, with no side inputs. Image produced using draw.io \cite  {jgraph2018draw}.}}{23}}
\newlabel{figure:approach_comparison}{{3.2}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Architecture of the generator. Each fully connected feed-forward layer's activation function (blue) is shown; the output layer (red) differs from the other layers in that it does not use the leaky ReLU activation, but rather a modulus function. Image produced using draw.io \cite  {jgraph2018draw}.}}{23}}
\newlabel{figure:architecture_generator}{{3.3}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Generative Model}{23}}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{jgraph2018draw}
\citation{numpy}
\citation{numpy}
\citation{tensorflow}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Convolutional discriminator architecture. The output of the generator is convolved multiple times in order to extract higher-level features from the sequence; this is followed by pooling to reduce the output size, and fully connected feed-forward layers to produce the final classification output. Image produced using draw.io \cite  {jgraph2018draw}.}}{24}}
\newlabel{figure:architecture_conv}{{3.4}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Convolutional predictor architecture. The design is shared with the discriminator, but the size of the input sequence and the meaning of the output scalar are different. The predictor produces a guess for the last value in the generator's output based on the previous values. Image produced using draw.io \cite  {jgraph2018draw}.}}{24}}
\newlabel{figure:architecture_conv}{{3.5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Discriminative Model}{24}}
\citation{bhatia2017why}
\citation{tensorflow2018intro}
\citation{tensorflow2018intro}
\citation{tensorflow2018graphs}
\citation{tensorflow2018intro}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Predictive Model}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Implementation Technologies}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction to TensorFlow}{25}}
\citation{tensorflow2018tutorials}
\citation{tensorflow2018tfgan}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Software Design}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Generative Model}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Discriminative and Predictive Models}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Connecting the Models Adversarially}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Obtaining Training Data}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Training Procedure for the Discriminative GAN}{28}}
\newlabel{subsection:training_disc}{{3.3.5}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Training Procedure for the Predictive GAN}{28}}
\newlabel{subsection:training_pred}{{3.3.6}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}Custom Tensor Operations}{28}}
\newlabel{subsection:custom_ops}{{3.3.7}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.8}Utilities}{28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{30}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:experiments}{{4}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experimental Procedure}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Training}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Statistical Testing}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Results}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Discriminative}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Predictive}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Computational Complexity}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Evaluation}{30}}
\newlabel{subsection:evaluation}{{4.4}{30}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:conclusion}{{5}{31}}
\bibstyle{plain}
\bibdata{references.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Further Investigation}{32}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{bhatia2017why}{1}
\bibcite{numpy}{2}
\bibcite{forbes2016short}{3}
\bibcite{tensorflow}{4}
\bibcite{tensorflow2018graphs}{5}
\bibcite{tensorflow2018intro}{6}
\bibcite{tensorflow2018tutorials}{7}
\bibcite{abadi2016learning}{8}
\bibcite{albelwi2017framework}{9}
\bibcite{amazon2018aws}{10}
\bibcite{anderson2010security}{11}
\bibcite{barker2007recommendation}{12}
\bibcite{microsoft2018azure}{13}
\bibcite{cover2012elements}{14}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{deng2017developments}{15}
\bibcite{desai2011pseudo}{16}
\bibcite{desai2012pseudo}{17}
\bibcite{donald1998art}{18}
\bibcite{ferguson2010cryptography}{19}
\bibcite{goodfellow2016nips}{20}
\bibcite{goodfellow2016deep}{21}
\bibcite{goodfellow2014generative}{22}
\bibcite{karpathy2017cs231n}{23}
\bibcite{katz2014introduction}{24}
\bibcite{kelsey1998cryptanalytic}{25}
\bibcite{kingma2014adam}{26}
\bibcite{klimov2002analysis}{27}
\bibcite{lavasani2009practical}{28}
\bibcite{google2018automl}{29}
\bibcite{google2018trends}{30}
\bibcite{jgraph2018draw}{31}
\bibcite{melicher2016fast}{32}
\bibcite{menezes1996handbook}{33}
\bibcite{rukhin2001statistical}{34}
\bibcite{russel2009artificial}{35}
\bibcite{sharma2017activation}{36}
\bibcite{tensorflow2018tfgan}{37}
\bibcite{terr2009math}{38}
\bibcite{tirdad2010hopfield}{39}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Training Data}{36}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Code Listings}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Generator}{37}}
\newlabel{appendix:generator}{{B.1}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Adversary, Convolutional Architecture}{37}}
\newlabel{appendix:convolutional}{{B.2}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Adversary, LSTM Architecture}{38}}
\newlabel{appendix:lstm}{{B.3}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Adversary, Convolutional LSTM Architecture}{38}}
\newlabel{appendix:convlstm}{{B.4}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {B.5}Training of Discriminative GAN}{39}}
\newlabel{appendix:training_discgan}{{B.5}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {B.6}Training of Predictive GAN}{39}}
\newlabel{appendix:training_predgan}{{B.6}{39}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  2B0F6F46CBA87E37AE5DD9B3B11BA1477C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F478FD3E27D1A397DBBF491DFD095CAC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CDDA6DC946A472082B95C9B35C935E717C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  241FCF33EAA2EA61285F36559116CBAD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA898703B7A047FEC91A00946BDB33767C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9687E97308B3EE6011878DF982BFA4727C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  470C1201BA4FAAB322FAC8FD6663615F7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EB844645E8E61DE0A4CF4B991E65E63E7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F3B9BACFCFEAD73E0AE49B341E8E3A527C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  FC158F94C02041F0DE8136BBDAFF87F37C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  30ABE6287F1B3112C6A42D605B84C0F87C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  DA933CDAC9C8F936D1675308EED9A1017C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  20FF3AB58C9CD8AD52E24501CC46C84C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9CDB0EFE09A8295DBCAF1E387E06C7157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  684B83286E503589996B08CBEC11F9A57C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9D468F0F487AD6BAC55B984E50CAF3BB7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F1C84F8167B4ED8A289218AC27A768DD7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  3815CAAAF6D678434BB0DD353CA1D8487C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  BC4E70DAC4A264E62A80B58427011B8D7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  F82D86802EB00649D1302D4B2AD0C11A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  090F3DDE3F4827A496BE04C4299DC1DF7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  4CABC0EE807D64644CF35F9838EE66157C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  7BB2296F2C23E3D8D42263634613906A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  CC8F4CB50C34B9656FB22C920A967C4C7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  38E5D2095B76575F55EB2D505DD5B4417C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  B721F2FD0C2484502B723AAEA86A00067C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  793EE9ECA4E3E41EA42F6648FE4D79027C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  9914B55C912FBA0669AE2AD56336B4C77C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  EBD91A97485EB6386993AB64D42487FE7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  E8B38AF19D2579281620EB8F04349BFC7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  5164D5E84F853507392851B55CBD98F77C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
